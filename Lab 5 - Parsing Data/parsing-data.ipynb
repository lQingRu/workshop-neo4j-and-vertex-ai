{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Parsing Data\n",
    "In this notebook, let's explore how to leverage generative AI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses Form-13 data from SEC EDGAR. This is partially structured data, a mix of text and XML.  Instead of spending our time writing a bespoke parser to extract data from these files and load into Neo4j, we can prompt a Large Language Model (LLM) to do this for us automatically.  We will then also use the LLM to generate Cypher statements to load the extracted data into a Neo4j graph.\n",
    "\n",
    "## Setup\n",
    "First, let's install the libraries we're going to need for this lab and the following notebook dependent labs.  We'll also want to reboot the kernel once done.  To do that, go to the \"Kernel\" menu and click \"Restart Kernel and Clear All Outputs.\"  That will get rid of everything the install statements printed, leaving us with a cleaner notebook to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad4e36-7048-4bbf-a71a-a5d1b24d0ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-24.2\n",
      "Collecting graphdatascience\n",
      "  Downloading graphdatascience-1.11-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (1.12)\n",
      "Collecting neo4j<6.0,>=4.4.12 (from graphdatascience)\n",
      "  Downloading neo4j-5.24.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (2.2.2)\n",
      "Collecting pyarrow<17.0,>=14.0.1 (from graphdatascience)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting textdistance<5.0,>=4.0 (from graphdatascience)\n",
      "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.63.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (2.32.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j<6.0,>=4.4.12->graphdatascience) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Downloading graphdatascience-1.11-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading neo4j-5.24.0-py3-none-any.whl (294 kB)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: textdistance, pyarrow, neo4j, graphdatascience\n",
      "Successfully installed graphdatascience-1.11 neo4j-5.24.0 pyarrow-16.1.0 textdistance-4.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pydantic==1.10.11\n",
      "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (148 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.11) (4.12.2)\n",
      "Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.9.0 requires pydantic>=2, but you have pydantic 1.10.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.10.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain==0.1.20\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (3.10.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.20)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain==0.1.20)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain==0.1.20)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.20)\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.20)\n",
      "  Downloading langsmith-0.1.118-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (2.32.3)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.20)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.20) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.20) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (3.7.1)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.20)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.2.2)\n",
      "Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.118-py3-none-any.whl (289 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, packaging, orjson, mypy-extensions, jsonpointer, httpcore, typing-inspect, marshmallow, jsonpatch, httpx, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "\u001b[33m  WARNING: The script httpx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.9.0 requires pydantic>=2, but you have pydantic 1.10.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 langsmith-0.1.118 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 packaging-23.2 tenacity-8.5.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.112.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio)\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
      "Collecting huggingface-hub>=0.19.3 (from gradio)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Collecting urllib3~=2.0 (from gradio)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.37.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.63.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Collecting pydantic-core==2.23.3 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, urllib3, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, importlib-resources, ffmpy, aiofiles, pydantic, huggingface-hub, gradio-client, gradio\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.11\n",
      "    Uninstalling pydantic-1.10.11:\n",
      "      Successfully uninstalled pydantic-1.10.11\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts gradio and upload_theme are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 huggingface-hub-0.24.6 importlib-resources-6.4.5 pydantic-2.9.1 pydantic-core-2.23.3 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.4 semantic-version-2.10.0 tomlkit-0.12.0 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting IProgress\n",
      "  Downloading IProgress-0.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from IProgress) (1.16.0)\n",
      "Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: IProgress\n",
      "Successfully installed IProgress-0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.63.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in /home/jupyter/.local/lib/python3.10/site-packages (0.0.38)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.10.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.1.52)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.1.118)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jupyter/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (2.23.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-google-vertexai\n",
      "  Downloading langchain_google_vertexai-1.0.10-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (1.60.0)\n",
      "Collecting google-cloud-storage<3.0.0,>=2.17.0 (from langchain-google-vertexai)\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-google-vertexai) (0.27.2)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-google-vertexai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain-core<0.3,>=0.2.33 (from langchain-google-vertexai)\n",
      "  Downloading langchain_core-0.2.39-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.32.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (23.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.0.5)\n",
      "Requirement already satisfied: pydantic<3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.9.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.16)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai)\n",
      "  Downloading google_api_core-2.19.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.4.1)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (1.5.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.7.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (0.1.118)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (4.12.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.63.2)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.65.4)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.13.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jupyter/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.33->langchain-google-vertexai) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.2.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.26.4)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.16.0)\n",
      "Downloading langchain_google_vertexai-1.0.10-py3-none-any.whl (86 kB)\n",
      "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.2.39-py3-none-any.whl (396 kB)\n",
      "Downloading google_api_core-2.19.2-py3-none-any.whl (139 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Installing collected packages: httpx-sse, google-resumable-media, google-api-core, langchain-core, google-cloud-storage, langchain-google-vertexai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.52\n",
      "    Uninstalling langchain-core-0.1.52:\n",
      "      Successfully uninstalled langchain-core-0.1.52\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.39 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.39 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.19.2 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.19.2 google-cloud-storage-2.18.2 google-resumable-media-2.7.2 httpx-sse-0.4.0 langchain-core-0.2.39 langchain-google-vertexai-1.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "%pip install --user graphdatascience\n",
    "%pip install --user \"pydantic==1.10.11\"\n",
    "%pip install --user \"langchain==0.1.20\"\n",
    "%pip install --user gradio\n",
    "%pip install --user IProgress\n",
    "%pip install --user tqdm\n",
    "%pip install --user langchain-community\n",
    "%pip install --user langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7c539-68b5-4126-990e-60c85b84fafa",
   "metadata": {},
   "source": [
    "Now restart the kernel. That will allow the Python evironment to import the new packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a385-8d3a-4c69-9817-ed14ba8e6511",
   "metadata": {},
   "source": [
    "## Prompt Definition\n",
    "We will extract knowledge adhering to the same schema we used previously.  To teach the LLM about the schema, we will use a series of prompts.  Each prompt is focused on only one task, extracting a specific entity:\n",
    "\n",
    "1. Manager Information\n",
    "2. Filing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeab081-6bc7-41a1-92a6-1f02b8adcf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mgr_info_tpl = \"\"\"From the text below, extract the following as json. Do not miss any of these information.\n",
    "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
    "* \"managerName\" - The name from the <name> tag under <filingManager> tag\n",
    "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
    "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
    "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
    "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
    "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
    "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
    "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2483efd5-fedf-49b0-9f01-534bd7390968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filing_info_tpl = \"\"\"The text below contains a list of investments. Each instance of <infoTable> tag represents a unique investment. \n",
    "For each investment, please extract the below variables into json then combine into a list enclosed by 3 backticks. Please use the quoted names below while doing this\n",
    "* \"cusip\" - The cusip from the <cusip> tag under <infoTable> tag\n",
    "* \"companyName\" - The name under the <nameOfIssuer> tag.\n",
    "* \"value\" - The value from the <value> tag under <infoTable> tag. Return as a number. \n",
    "* \"shares\" - The sshPrnamt from the <sshPrnamt> tag under <infoTable> tag. Return as a number. \n",
    "* \"sshPrnamtType\" - The sshPrnamtType from the <sshPrnamtType> tag under <infoTable> tag\n",
    "* \"investmentDiscretion\" - The investmentDiscretion from the <investmentDiscretion> tag under <infoTable> tag\n",
    "* \"votingSole\" - The votingSole from the <votingSole> tag under <infoTable> tag\n",
    "* \"votingShared\" - The votingShared from the <votingShared> tag under <infoTable> tag\n",
    "* \"votingNone\" - The votingNone from the <votingNone> tag under <infoTable> tag\n",
    "\n",
    "Output format:\n",
    "* DO NOT output XML tags in the response. The output should be a valid JSON list enclosed by 3 backticks\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481717bb-6db2-4465-8667-b0c07da381d5",
   "metadata": {},
   "source": [
    "## Functions for Using LLMs\n",
    "Let's create some helper function to talk to the LLM with our prompt and text input. \n",
    "\n",
    "The [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models) describes the available foundation models.  We will use gemini-1.5-flash model. In some cases, there may be a need to fine-tune LLM models for domain specific use cases. [Vertex AI provides an elegant way to fine-tune](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models) where the updated weights/model stay within your tenant and the base model is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e42db05-86be-4171-9ef3-538af376057e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# Wrapper for calling language model\n",
    "def run_text_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_output_tokens: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    model = GenerativeModel(model_name)\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={ \"temperature\": temperature,\n",
    "                           \"max_output_tokens\": max_output_tokens,\"top_p\": top_p,\"top_k\": top_k,},)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ca2aab-87a9-4628-9f17-6a8fd7d08153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper for entity extraction and parsing\n",
    "def extract_entities_relationships(prompt):\n",
    "    try:\n",
    "        res = run_text_model(\"gemini-1.5-flash-001\", 0, 1024, 0.8, 1, prompt)\n",
    "        res = res.split('```')[1].strip('json').replace('\\n', ' ')\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726ef3e9-e66f-45ed-be59-d386ce1db4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# splitting function for chunking up filing information to avoid hitting LLM token limits\n",
    "def split_filing_info(s, chunk_size=5):\n",
    "    pattern = '(</(\\w+:)?infoTable>)'\n",
    "    splitter = re.findall(pattern, s)[0][0]\n",
    "    _parts = s.split(splitter)\n",
    "    if len(_parts) > chunk_size:\n",
    "        chunks_of_list = np.array_split(_parts, len(_parts)/chunk_size) # max 5 filings per part\n",
    "        chunks_of_str = map(lambda x: splitter.join(x)+splitter, chunks_of_list)\n",
    "        l = list(chunks_of_str)\n",
    "        if len(l) > 0:\n",
    "            l[len(l)-1] = re.sub(f'{splitter}$', '', l[len(l)-1])\n",
    "        return l\n",
    "    else:\n",
    "        return [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9dee8-c5dc-42ad-b05d-0f0d7e11d551",
   "metadata": {},
   "source": [
    "## Test Example for Parsing\n",
    "Let's start with one Form 13 file to see how we can parse it with Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cc2dfa-4947-4afc-8b2a-6cd16338029a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('neo4j-datasets')\n",
    "blob = bucket.blob('hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1026200_0001567619-22-000057.txt')\n",
    "\n",
    "inp_text = blob.download_as_string().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82c1ec-d49d-4444-b4fe-bc0be2777308",
   "metadata": {},
   "source": [
    "We can take a look at the file.  Note that it is an oddball mix of XML, delimeted and fixed spacing formatting that no standard parser could make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33cda3eb-25c6-47cf-8d11-e1981766aeae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0001567619-22-000057.txt : 20220103\n",
      "<SEC-HEADER>0001567619-22-000057.hdr.sgml : 20220103\n",
      "<ACCEPTANCE-DATETIME>20220103171622\n",
      "ACCESSION NUMBER:\t\t0001567619-22-000057\n",
      "CONFORMED SUBMISSION TYPE:\t13F-HR\n",
      "PUBLIC DOCUMENT COUNT:\t\t2\n",
      "CONFORMED PERIOD OF REPORT:\t20211231\n",
      "FILED AS OF DATE:\t\t20220103\n",
      "DATE AS OF CHANGE:\t\t20220103\n",
      "EFFECTIVENESS DATE:\t\t20220103\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tPRIVATE ASSET MANAGEMENT INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001026200\n",
      "\t\tIRS NUMBER:\t\t\t\t330524568\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tCA\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t13F-HR\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t028-03581\n",
      "\t\tFILM NUMBER:\t\t22503075\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t5348 CARROLL CANYON RD\n",
      "\t\tSTREET 2:\t\t#200\n",
      "\t\tCITY:\t\t\tSAN DIEGO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t92121\n",
      "\t\tBUSINESS PHONE:\t\t6197923800\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t5348 CARROLL CANYON RD\n",
      "\t\tSTREET 2:\t\t#200\n",
      "\t\tCITY:\t\t\tSAN DIEGO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t92121\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>13F-HR\n",
      "<SEQUENCE>1\n",
      "<FILENAME>primary_doc.xml\n",
      "<TEXT>\n",
      "<XML>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:com=\"http://www.sec.gov/edgar/common\">\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      <flags>\n",
      "        <confirmingCopyFlag>false</confirmingCopyFlag>\n",
      "        <returnCopyFlag>true</returnCopyFlag>\n",
      "        <overrideInternetFlag>false</overrideInternetFlag>\n",
      "      </flags>\n",
      "      <f\n"
     ]
    }
   ],
   "source": [
    "print(inp_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41717f-0c6e-4642-b6fd-ca327ce474cf",
   "metadata": {},
   "source": [
    "We can split data into manager and filing info pieces using `<XML>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f23c86-3274-40fd-9c97-70337d8dd535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = inp_text.split('<XML>')\n",
    "manager_info = contents[1].split('</XML>')[0].strip()\n",
    "filing_info = contents[2].split('</XML>')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21286ffa-2ff2-40cd-8eee-8b4626f32c1f",
   "metadata": {},
   "source": [
    "## Parsing Manager Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522d7f7b-5b24-4d05-b0b6-a3d8a85dcad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed35d7c5-ef70-419b-b9c3-69c34003a75e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the text below, extract the following as json. Do not miss any of these information.\n",
      "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
      "* \"managerName\" - The name from the <name> tag under <filingManager> tag\n",
      "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
      "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
      "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
      "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
      "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
      "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
      "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
      "\n",
      "Text:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:com=\"http://www.sec.gov/edgar/common\">\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      <flags>\n",
      "        <confirmingCopyFlag>false</confirmingCopyFlag>\n",
      "        <returnCopyFlag>true</returnCopyFlag>\n",
      "        <overrideInternetFlag>false</overrideInternetFlag>\n",
      "      </flags>\n",
      "      <filer>\n",
      "        <credentials>\n",
      "          <cik>0001026200</cik>\n",
      "          <ccc>XXXXXXXX</ccc>\n",
      "        </credentials>\n",
      "      </filer>\n",
      "      <periodOfReport>12-31-2021</periodOfReport>\n",
      "    </filerInfo>\n",
      "  </headerData>\n",
      "  <formData>\n",
      "    <coverPage>\n",
      "      <reportCalendarOrQuarter>12-31-2021</reportCalendarOrQuarter>\n",
      "      <filingManager>\n",
      "        <name>PRIVATE ASSET MANAGEMENT INC</name>\n",
      "        <address>\n",
      "          <com:street1>5348 CARROLL CANYON RD</com:street1>\n",
      "          <com:street2>#200</com:street2>\n",
      "          <com:city>SAN DIEGO</com:city>\n",
      "          <com:stateOrCountry>CA</com:stateOrCountry>\n",
      "          <com:zipCode>92121</com:zipCode>\n",
      "        </address>\n",
      "      </filingManager>\n",
      "      <reportType>13F HOLDINGS REPORT</reportType>\n",
      "      <form13FFileNumber>028-03581</form13FFileNumber>\n",
      "      <provideInfoForInstruction5>N</provideInfoForInstruction5>\n",
      "    </coverPage>\n",
      "    <signatureBlock>\n",
      "      <name>Michael D. Berlin</name>\n",
      "      <title>General Counsel</title>\n",
      "      <phone>858-750-4209</phone>\n",
      "      <signature>/s/ Michael D. Berlin</signature>\n",
      "      <city>San Diego</city>\n",
      "      <stateOrCountry>CA</stateOrCountry>\n",
      "      <signatureDate>01-03-2022</signatureDate>\n",
      "    </signatureBlock>\n",
      "    <summaryPage>\n",
      "      <otherIncludedManagersCount>0</otherIncludedManagersCount>\n",
      "      <tableEntryTotal>149</tableEntryTotal>\n",
      "      <tableValueTotal>814509</tableValueTotal>\n",
      "      <isConfidentialOmitted>false</isConfidentialOmitted>\n",
      "    </summaryPage>\n",
      "  </formData>\n",
      "</edgarSubmission>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b13100f-de4b-49f9-89f6-58e01c19727e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cusip': '39818P880',\n",
       "  'companyName': 'Griffin Cap Essntl Ast Reit II Com Cl E',\n",
       "  'value': 157,\n",
       "  'shares': 17256,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 17256},\n",
       " {'cusip': '46564T107',\n",
       "  'companyName': 'Iteris Inc',\n",
       "  'value': 80,\n",
       "  'shares': 20000,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 20000},\n",
       " {'cusip': '90137F103',\n",
       "  'companyName': '22nd Century Group Inc',\n",
       "  'value': 56,\n",
       "  'shares': 18000,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 18000},\n",
       " {'cusip': 'Q05532298',\n",
       "  'companyName': 'Artemis Resources Ltd',\n",
       "  'value': 1,\n",
       "  'shares': 10714,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 10714}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Use LLM to parse out manager information\n",
    "manager_data = json.loads(extract_entities_relationships(prompt))\n",
    "manager_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775186c0-d2bc-4711-88c4-0e61488ad2bd",
   "metadata": {},
   "source": [
    "## Parse Filing Information\n",
    "We will parse filing info in a similar manner to manager information. Because the filings include a list of many entries however, we will want to split the input into chunks so as not to exceed input or output token limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa1690a-012b-4056-8fcc-6e04e4bb26f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_info_chunks = split_filing_info(filing_info)\n",
    "len(filing_info_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "125ed4c0-22b9-4fc8-83f0-a1ccc5eaa112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cusip': '747525103', 'companyName': 'Qualcomm Inc.', 'value': 93241, 'shares': 509872, 'sshPrnamtType': 'SH', 'investmentDiscretion': 'SOLE', 'votingSole': 0, 'votingShared': 0, 'votingNone': 509872}, {'cusip': '037833100', 'companyName': 'Apple Inc', 'value': 47051, 'shares': 264971, 'sshPrnamtType': 'SH', 'investmentDiscretion': 'SOLE', 'votingSole': 0, 'votingShared': 0, 'votingNone': 264971}, {'cusip': '594918104', 'companyName': 'Microsoft Corp', 'value': 37188, 'shares': 110568, 'sshPrnamtType': 'SH', 'investmentDiscretion': 'SOLE', 'votingSole': 0, 'votingShared': 0, 'votingNone': 110568}, {'cusip': '02079K107', 'companyName': 'Alphabet Inc C', 'value': 32478, 'shares': 11224, 'sshPrnamtType': 'SH', 'investmentDiscretion': 'SOLE', 'votingSole': 0, 'votingShared': 0, 'votingNone': 11224}, {'cusip': '437076102', 'companyName': 'The Home Depot Inc', 'value': 32006, 'shares': 77119, 'sshPrnamtType': 'SH', 'investmentDiscretion': 'SOLE', 'votingSole': 0, 'votingShared': 0, 'votingNone': 77119}]\n"
     ]
    }
   ],
   "source": [
    "prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunks[0])\n",
    "response = json.loads(extract_entities_relationships(prompt))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2560b-2e6c-4b96-abc3-7c96196d0eb5",
   "metadata": {},
   "source": [
    "## Test Example\n",
    "\n",
    "Let's walk through the steps to do this with just the 1 form above first, then we can move on to parsing and ingesting multiple form13s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a67de-b18f-4c65-a529-b890086663b8",
   "metadata": {},
   "source": [
    "To start we can run the LLM parsing over all the filing info from the form and then combine the resulting JSON into a list conducive for Neo4j loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827621eb-2f7d-4ef4-9f00-cddbc5e2cfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cusip': '747525103',\n",
       "  'companyName': 'Qualcomm Inc.',\n",
       "  'value': 93241,\n",
       "  'shares': 509872,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 509872,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '037833100',\n",
       "  'companyName': 'Apple Inc',\n",
       "  'value': 47051,\n",
       "  'shares': 264971,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 264971,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '594918104',\n",
       "  'companyName': 'Microsoft Corp',\n",
       "  'value': 37188,\n",
       "  'shares': 110568,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 110568,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '02079K107',\n",
       "  'companyName': 'Alphabet Inc C',\n",
       "  'value': 32478,\n",
       "  'shares': 11224,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 11224,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '437076102',\n",
       "  'companyName': 'The Home Depot Inc',\n",
       "  'value': 32006,\n",
       "  'shares': 77119,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 77119,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filings_list = []\n",
    "import time\n",
    "for filing_info_chunk in filing_info_chunks:\n",
    "    prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "    response = extract_entities_relationships(prompt)\n",
    "    # time.sleep(2) #uncomment this line if you face any rate limit error\n",
    "    if '```' in response:\n",
    "        response = response.split('```')[1].strip('json')\n",
    "    filings_list.extend(json.loads(response))\n",
    "\n",
    "for item in filings_list:\n",
    "    item['managerName'] = manager_data['managerName']\n",
    "    item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "filings_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f6b661-7d08-42fa-9dda-6ea9e8dfcffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b83ca6-c44b-4f43-9dcd-c88cf525a978",
   "metadata": {},
   "source": [
    "## Establish Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93fffd14-e6bb-4843-90e9-d1d4e7cef3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://7e098dc5.databases.neo4j.io' #Eg 'neo4j+s://ccc5f4f5.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = '19_Kt_bHLte_0F2r4OAqGv1ihz9ciXJfJaav0-hpNrI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f38e2ef1-f5bd-4561-87e7-a014b8af2e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6adf6-a299-47ca-a965-6b6cdec67f77",
   "metadata": {},
   "source": [
    "Before loading, we should create node key constraints for nodes.  This acts as a unique id and an index and is necessary for fast, efficient queries.  In general, if you notice ingestion is super slow (and getting slower) with Neo4j, double-check that you created indexes.  For this small sample, it won't matter, but it will undoubtedly impact as we ingest more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3084e75-da30-42c1-8ca4-10cf55e7c3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_manager IF NOT EXISTS FOR (n:Manager) REQUIRE (n.managerName) IS NODE KEY')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE (n.cusip) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814834e0-54f6-4e6f-9a6e-67a526cf3bf6",
   "metadata": {},
   "source": [
    "To merge the data, we can use parameterized Cypher queries.  Basically, we will send filings in batches (in this sample case, just one batch) for each node and relationship type and insert them as parameters in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb25f22-39a0-4930-bf05-8adaa45c2f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_node_merge_count\n",
       "0                       149"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge company nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (c:Company {cusip: record.cusip})\n",
    "SET c.companyName = record.companyName\n",
    "RETURN count(c) AS company_node_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "635b9393-a2b6-4da6-94a4-cca6bcf515bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_node_merge_count\n",
       "0                         1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge manager node\n",
    "gds.run_cypher('''\n",
    "MERGE (m:Manager {managerName: $name})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'name':manager_data['managerName']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5394e270-45c3-44c9-96bb-6ea210dbb28a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owns_relationship_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   owns_relationship_merge_count\n",
       "0                            149"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge owns Relationship\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MATCH (m:Manager {managerName: record.managerName})\n",
    "MATCH (c:Company {cusip: record.cusip})\n",
    "MERGE(m)-[r:OWNS]->(c)\n",
    "SET r.reportCalendarOrQuarter = date(datetime({epochmillis: apoc.date.parse(record.reportCalendarOrQuarter, \"ms\", \"MM-dd-yyyy\")})),\n",
    "    r.value = record.value,\n",
    "    r.shares = record.shares\n",
    "RETURN count(r) AS owns_relationship_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ebde6-7e39-40fc-8457-4d39bb45efd1",
   "metadata": {},
   "source": [
    "## Ingest Multiple Form 13 Files\n",
    "We will make a pipeline using the methods above.  In this case we will take a two-step approach, first parse all the data, then chunk that data and ingest into Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0900b-0312-4213-9a3d-13e512226f93",
   "metadata": {},
   "source": [
    "For purposes of this lab we will just use a few form13 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29c1d593-dd40-4f08-ac0d-9c8e9ff201d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### If you have time to parse more files, you can uncomment these lines.\n",
    "sample_file_names = [\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1844571_0001844571-22-000001.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1875995_0001875995-22-000004.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-06_archives_edgar_data_1495703_0001495703-22-000002.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568c27a4-d6bf-4e12-af6e-478e20fe5ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for getting filing info\n",
    "def get_manager_and_filing_info(raw_txt):\n",
    "    contents = raw_txt.split('<XML>')\n",
    "    manager_info = contents[1].split('</XML>')[0].strip()\n",
    "    filing_info = contents[2].split('</XML>')[0].strip()\n",
    "    \n",
    "    return manager_info, filing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d46cc84-53b5-4088-a5aa-578f7eb5d0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing 3 Form 13 Files ===\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1844571_0001844571-22-000001.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1875995_0001875995-22-000004.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-06_archives_edgar_data_1495703_0001495703-22-000002.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "CPU times: user 574 ms, sys: 117 ms, total: 691 ms\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'=== Parsing {len(sample_file_names)} Form 13 Files ===')\n",
    "\n",
    "filings_list = []\n",
    "manager_list = []\n",
    "\n",
    "for file_name in sample_file_names:\n",
    "    \n",
    "    print(f'--- parsing {file_name} ---')\n",
    "    try:\n",
    "        # Get raw form13 file\n",
    "        print('getting file text from gcloud....')\n",
    "        blob = bucket.blob(file_name)\n",
    "        raw_text = blob.download_as_string().decode()\n",
    "\n",
    "        # Get raw manager and filing info from file\n",
    "        print('getting file contents...')\n",
    "        manager_info, filing_info = get_manager_and_filing_info(raw_text)\n",
    "\n",
    "        # Parse manager info into dict using LLM\n",
    "        print('Parsing submission and manager info...')\n",
    "        mng_prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "        mng_response = extract_entities_relationships(mng_prompt)\n",
    "        manager_data = json.loads(mng_response.replace('```', ''))\n",
    "        manager_list.append({'managerName': manager_data['managerName']})\n",
    "\n",
    "        # Parse filing info into list of dicts using LLM\n",
    "        print('Parsing filing info...')\n",
    "        tmp_filing_list = []\n",
    "        for filing_info_chunk in split_filing_info(filing_info):\n",
    "            filing_prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "            filing_response = extract_entities_relationships(filing_prompt)\n",
    "            #time.sleep(3) #uncomment this line if you face any rate limit error\n",
    "            if '```' in filing_response:\n",
    "                filing_response = filing_response.split('```')[1].strip('json')\n",
    "            tmp_filing_list.extend(json.loads(filing_response))\n",
    "        for item in tmp_filing_list: #Add information from manager_info to enable OWNS relationship loading\n",
    "            item['managerName'] = manager_data['managerName']\n",
    "            item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "        filings_list.extend(tmp_filing_list)\n",
    "    except Exception as e:\n",
    "        print(filing_response)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656917-8b98-4b94-a905-fcd4a4a5f8bd",
   "metadata": {},
   "source": [
    "Now we can merge the manager nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df9206bb-d619-4649-99cf-f5939461555a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_node_merge_count\n",
       "0                         3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge manager nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (m:Manager {managerName: record.managerName})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'records':manager_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c27c4-4e48-4a08-8aba-26a3874e7f3e",
   "metadata": {},
   "source": [
    "For filings lets check ther length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c31f300-1c53-4795-ae1c-7228f240ddf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ede58-e686-42c8-90f7-fd8919a17f36",
   "metadata": {},
   "source": [
    "While we should not need chunking for this example, below is an example of how to chunk up a parameterized function for loading in case you need to scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceb716e1-c71c-4992-b654-016f9496f08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As the dataset gets bigger we will want to chunk up the filings we send to Neo4j\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2216015-d8ca-475d-8240-a5717081c8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company_node_merge_count\n",
      "0                       140\n"
     ]
    }
   ],
   "source": [
    "# Merge company nodes\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MERGE (c:Company {cusip: record.cusip})\n",
    "    SET c.companyName = record.companyName\n",
    "    RETURN count(c) AS company_node_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "750215eb-ec16-4b42-909f-cf763b9955b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   owns_relationship_merge_count\n",
      "0                            202\n"
     ]
    }
   ],
   "source": [
    "# Merge owns Relationships\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH (m:Manager {managerName: record.managerName})\n",
    "    MATCH (c:Company {cusip: record.cusip})\n",
    "    MERGE(m)-[r:OWNS]->(c)\n",
    "    SET r.reportCalendarOrQuarter = date(datetime({epochmillis: apoc.date.parse(record.reportCalendarOrQuarter, \"ms\", \"MM-dd-yyyy\")})),\n",
    "        r.value = record.value,\n",
    "        r.shares = record.shares\n",
    "    RETURN count(r) AS owns_relationship_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88924cd-b43a-42ca-b975-87f1362987fc",
   "metadata": {},
   "source": [
    "This type of workflow can be applied to other unstructured data to parse entities and relationships with language models and load them into a Neo4j knowledge graph. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
